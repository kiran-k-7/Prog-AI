# -*- coding: utf-8 -*-
"""prog for Data CA-Kiran.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wx2OBZE2-1DWZDLNJINAZ80EZdghQw06

Here I am scrapping data from Wikepedia to get earthquake data for detecting seismic activity in Japan

**Step 1:Data Scrapping**
"""

#importing libraries to scrap data
import requests
from bs4 import BeautifulSoup

def scarpe_seismic_data():
  url ="https://www.seismicportal.eu/"

  headers ={
      "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36"
  }

  response=requests.get(url,headers=headers)

  if response.status_code==200:
    soup= BeautifulSoup(response.content, 'html.parser')

    earthquake_table=soup.find('table',{'id':'earthquakes'})

    #extracting the data available

    earthquake_data=[]
    if earthquake_table:
      rows=earthquake_table.find_all('tr')
      for row in rows:
        columns=row.find_all('td')

        earthquake_info={
            'Magnitude':columns[0].text.strip(),
            'Datatime':columns[1].text.strip(),
            'Lat':columns[2].text.strip(),
            'Lot':columns[3].text.strip(),
            'Region':columns[4].text.strip(),
            'Depth':columns[5].text.strip(),
            'Auth':columns[6].text.strip()
        }
        earthquake_data.append(earthquake_info)

    return earthquake_data

#printing the fetched data in the collab

earthquake_data = scarpe_seismic_data()
if earthquake_data:
    for quake in earthquake_data:
        print(quake)

x=scarpe_seismic_data()
print(x)

url = 'https://en.wikipedia.org/wiki/List_of_earthquakes_in_Japan'

page = requests.get(url)

soup = BeautifulSoup(page.text, 'html')

print(soup)

soup.find_all('table', class_ = 'wikitable sortable')

table=soup.find_all('table')[1]
print(table)

country_titles=soup.find_all('th')
print(country_titles)

country_table_titles=[title.text.strip() for title in country_titles]

print(country_table_titles)

original_array = country_table_titles
indices_to_delete = [8,9,10,11,12,13,14,15,16,17,18]  # Indices of elements to delete
updated_array = [original_array[i] for i in range(len(original_array)) if i not in indices_to_delete]
print(updated_array)

import pandas as pd

df= pd.DataFrame(columns= updated_array)

df

column_data=table.find_all('tr')

for row in column_data[1:]:
  row_data=row.find_all('td')
  i_raw_data=[data.text.strip() for data in row_data]
  print(i_raw_data)

  length=len(df)
  df.loc[length]=i_raw_data

df.tail()

columns_drop=['Name in Kanji']
df=df.drop(columns=columns_drop)
df

column_position=4
df.drop(df.columns[column_position],axis=1,inplace=True)
df

import os

directory = '/Users/kirankishore'

file_path = os.path.join(directory, 'Japan.csv')
df.to_csv(file_path, index=False)
print(file_path)

df.to_csv('Japan.csv')

from pandas import read_csv
data=read_csv('Japan.csv')
data.head()

